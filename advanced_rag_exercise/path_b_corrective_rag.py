#!/usr/bin/env python3
"""ğŸ”´ Path B: Corrective RAG (CRAG) â€” Hard Â· ~55 new lines

Build a self-correcting retrieval loop: Retrieve â†’ Grade â†’ Retry if needed.

You will:
  1. Build rewrite_query()     â€” LLM rewrites a poor query for better retrieval
  2. Build grade_documents()   â€” use reranker scores as relevance grades (0-1)
  3. Build corrective_rag()    â€” the full retrieve â†’ grade â†’ retry loop
  4. Wire up CLI               â€” --query and --threshold flags

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
KEY INSIGHT: RERANKER AS GRADER
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Traditional CRAG uses an LLM to grade each document ("Is this relevant?").
That's SLOW â€” one LLM call per document!

Our trick: use the reranker's relevance_score (0.0 to 1.0) as a grade.
  â€¢ score > 0.5  â†’ relevant âœ…
  â€¢ score < 0.3  â†’ irrelevant âŒ â†’ rewrite query and retry
This is 10x faster than LLM grading!

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
REUSING YOUR PART 1-3 CODE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  â€¢ _hybrid_search(conn, query, top_k)  â€” your RRF fusion search (Part 3)
  â€¢ _generate_answer(query, chunks)     â€” LLM generation (Part 3)

NEW TODAY (Remote GPU):
  â€¢ chat_client.chat.completions.create()  â€” remote LLM for query rewriting
  â€¢ _rerank(query, documents, top_k)       â€” remote reranker as document grader

Run with (from the workshop/ folder):
    uv run --no-project --with duckdb --with ollama --with openai --with requests \
        advanced_rag_exercise/path_b_corrective_rag.py \
        --query "How does attention work in transformers?" --threshold 0.3

Prerequisites:
  â€¢ workshop/docling_part3.py must exist (cp your Part 3 exercise)
  â€¢ docling-exercise-example-answers/output/rag_chunks.duckdb from Part 2
  â€¢ Ollama running with qwen3-embedding:0.6b
"""

from __future__ import annotations

import argparse
import sys
import time
from pathlib import Path

# â”€â”€ Make Part 3 importable â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# NOTE: This imports from the example answer key. To use YOUR OWN Part 3 code
# instead, change the two lines below:
#   sys.path.insert(0, str(Path(__file__).resolve().parent.parent / "docling-exercise"))
#   from docling_part3_exercise import (  â† rename your file to use underscores!)
# (Python can't import filenames with hyphens, so rename
#  docling-part3-exercise.py â†’ docling_part3_exercise.py first)
sys.path.insert(0, str(Path(__file__).resolve().parent.parent / "docling-exercise-example-answers"))

from docling_part3_answer import (  # â† change to docling_part3_exercise to use YOUR code (see NOTE above)
    _connect_db, _hybrid_search, _generate_answer,
    _add_embeddings, _create_indexes,
)
from openai import OpenAI
import requests

# â”€â”€ Remote GPU Models (provided â€” no changes needed) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
chat_client = OpenAI(
    base_url="https://dev-8--vllm-qwen3-vl-8b-serve.modal.run/v1",
    api_key="not-needed",
)
CHAT_MODEL = "qwen3-vl-8b"

RERANK_URL = "https://dev-8--qwen3-vl-reranker-2b-serve.modal.run/v1/rerank"
RERANK_MODEL = "qwen3-vl-reranker-2b"

def _rerank(query: str, documents: list[str], top_k: int = 3) -> list[tuple[str, float]]:
    """Rerank documents using remote GPU reranker. Returns [(text, score), ...]."""
    resp = requests.post(RERANK_URL, json={
        "model": RERANK_MODEL, "query": query,
        "documents": documents, "top_n": top_k,
    })
    results = resp.json()["results"]
    return [(r["document"], r["relevance_score"]) for r in results]

# â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_DB = Path(__file__).resolve().parent.parent / "docling-exercise-example-answers" / "output" / "rag_chunks.duckdb"
MAX_RETRIES = 2  # Maximum number of query rewrites


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TODO 1 (â˜…â˜…): Build rewrite_query()  â€” ~10 lines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# When retrieved documents score poorly, the query itself may be the problem.
# Ask the LLM to rewrite it for better retrieval.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def rewrite_query(original_query: str) -> str:
    """Ask the remote LLM to rewrite a query for better retrieval."""

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  TODO 1: Rewrite the query using the remote LLM.             â”‚
    # â”‚                                                              â”‚
    # â”‚  Step A: Build a rewrite prompt                              â”‚
    # â”‚    - "Rewrite this search query to be more specific and      â”‚
    # â”‚      likely to find relevant documents: {original_query}"    â”‚
    # â”‚    - "Return ONLY the rewritten query, nothing else."        â”‚
    # â”‚                                                              â”‚
    # â”‚  Step B: Call chat_client.chat.completions.create()          â”‚
    # â”‚    - model=CHAT_MODEL                                        â”‚
    # â”‚    - Extract: resp.choices[0].message.content.strip()        â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    raise NotImplementedError("TODO 1: implement rewrite_query()")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TODO 2 (â˜…â˜…â˜…): Build grade_documents()  â€” ~15 lines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Use the reranker as a GRADER. Instead of asking an LLM "is this relevant?"
# (slow!), we use the reranker's score as a relevance signal.
#
# Score interpretation:
#   > threshold (e.g. 0.3) â†’ relevant âœ… keep it
#   â‰¤ threshold             â†’ irrelevant âŒ discard it
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def grade_documents(query: str, chunks: list[dict],
                    threshold: float = 0.3) -> tuple[list[dict], float]:
    """Grade documents using reranker scores. Returns (good_chunks, avg_score)."""

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  TODO 2: Grade documents using the reranker.                 â”‚
    # â”‚                                                              â”‚
    # â”‚  Step A: Extract texts from chunks                           â”‚
    # â”‚    - texts = [c["text"] for c in chunks]                     â”‚
    # â”‚                                                              â”‚
    # â”‚  Step B: Rerank ALL documents (not just top_k)               â”‚
    # â”‚    - scored = _rerank(query, texts, top_k=len(texts))        â”‚
    # â”‚    - This gives us scores for every document                 â”‚
    # â”‚                                                              â”‚
    # â”‚  Step C: Filter by threshold                                 â”‚
    # â”‚    - good = [{"text": t, "score": s} for t, s in scored      â”‚
    # â”‚             if s >= threshold]                                â”‚
    # â”‚    - avg_score = mean of all scores                          â”‚
    # â”‚                                                              â”‚
    # â”‚  Step D: Return (good_chunks, avg_score)                     â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    raise NotImplementedError("TODO 2: implement grade_documents()")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TODO 3 (â˜…â˜…â˜…â˜…): Build corrective_rag()  â€” ~25 lines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# The CRAG loop: retrieve â†’ grade â†’ if bad, rewrite query â†’ retry.
#
# Loop logic:
#   1. Search with current query
#   2. Grade the results
#   3. If avg_score >= threshold â†’ good enough, generate answer
#   4. If avg_score < threshold  â†’ rewrite query, go to step 1
#   5. After MAX_RETRIES, use whatever we have
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def corrective_rag(conn, query: str, threshold: float = 0.3,
                   top_k: int = 5) -> list[dict]:
    """CRAG loop: retrieve â†’ grade â†’ rewrite if needed â†’ retry."""

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  TODO 3: Implement the CRAG loop.                            â”‚
    # â”‚                                                              â”‚
    # â”‚  current_query = query                                       â”‚
    # â”‚  for attempt in range(MAX_RETRIES + 1):                      â”‚
    # â”‚                                                              â”‚
    # â”‚    Step A: Retrieve                                          â”‚
    # â”‚      - chunks = _hybrid_search(conn, current_query, top_k)   â”‚
    # â”‚      - Print: "ğŸ” Attempt {attempt+1}: searching..."         â”‚
    # â”‚                                                              â”‚
    # â”‚    Step B: Grade                                             â”‚
    # â”‚      - good, avg = grade_documents(current_query, chunks,    â”‚
    # â”‚                                    threshold)                â”‚
    # â”‚      - Print: "ğŸ“Š Avg score: {avg:.3f}, Good: {len(good)}"  â”‚
    # â”‚                                                              â”‚
    # â”‚    Step C: Decide                                            â”‚
    # â”‚      - If avg >= threshold OR last attempt: return good      â”‚
    # â”‚        (or chunks if no good ones)                           â”‚
    # â”‚      - Else: rewrite query and continue loop                 â”‚
    # â”‚        new_q = rewrite_query(current_query)                  â”‚
    # â”‚        Print: "ğŸ”„ Rewriting: '{current_query}' â†’ '{new_q}'" â”‚
    # â”‚        current_query = new_q                                 â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    raise NotImplementedError("TODO 3: implement corrective_rag()")



# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TODO 4 (â˜…): Wire up CLI  â€” ~15 lines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Add --query and --threshold flags.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main() -> int:
    parser = argparse.ArgumentParser(
        description="ğŸ”´ Path B: Corrective RAG â€” Retrieve â†’ Grade â†’ Retry"
    )
    parser.add_argument("--db", type=Path, default=DEFAULT_DB)
    parser.add_argument("--query", type=str, required=True)
    parser.add_argument("--threshold", type=float, default=0.3)

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  TODO 4: Wire up the pipeline.                               â”‚
    # â”‚                                                              â”‚
    # â”‚  - results = corrective_rag(conn, query, threshold)          â”‚
    # â”‚  - _generate_answer(query, results)                          â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    args = parser.parse_args()

    try:
        conn = _connect_db(args.db)
        _add_embeddings(conn)
        _create_indexes(conn)

        # TODO 4: Call corrective_rag() and _generate_answer() here
        raise NotImplementedError("TODO 4: wire up CLI")

        conn.close()
        return 0
    except NotImplementedError as exc:
        print(f"\nâš ï¸  {exc}", file=sys.stderr)
        return 1
    except Exception as exc:
        print(f"\nâŒ Error: {exc}", file=sys.stderr)
        return 1


if __name__ == "__main__":
    raise SystemExit(main())