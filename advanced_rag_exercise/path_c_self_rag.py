#!/usr/bin/env python3
"""ğŸ”´ Path C: Self-RAG â€” Hardest Â· ~50 new lines

Build a self-reflective generation loop: Generate â†’ Reflect â†’ Improve.

You will:
  1. Build generate_draft()      â€” generate an initial answer from retrieved context
  2. Build reflect_on_answer()   â€” LLM critiques its own answer for gaps/errors
  3. Build self_rag()            â€” the full generate â†’ reflect â†’ improve loop
  4. Wire up CLI                 â€” --query and --max-rounds flags

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
KEY INSIGHT: SELF-REFLECTION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Standard RAG: retrieve â†’ generate â†’ done (hope for the best!)
Self-RAG:     retrieve â†’ generate â†’ REFLECT â†’ improve â†’ repeat

The LLM checks its OWN answer:
  "Is this answer complete? Does it address the question? Any gaps?"
If the reflection finds issues, we retrieve MORE context and try again.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
REUSING YOUR PART 1-3 CODE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  â€¢ _hybrid_search(conn, query, top_k)  â€” your RRF fusion search (Part 3)

NEW TODAY (Remote GPU):
  â€¢ chat_client.chat.completions.create()  â€” remote 8B LLM for generation + reflection

Run with (from the workshop/ folder):
    uv run --no-project --with duckdb --with ollama --with openai --with requests \
        advanced_rag_exercise/path_c_self_rag.py \
        --query "How does attention work in transformers?" --max-rounds 3

Prerequisites:
  â€¢ workshop/docling_part3.py must exist (cp your Part 3 exercise)
  â€¢ docling-exercise-example-answers/output/rag_chunks.duckdb from Part 2
  â€¢ Ollama running with qwen3-embedding:0.6b
"""

from __future__ import annotations

import argparse
import sys
import time
from pathlib import Path

# â”€â”€ Make Part 3 importable â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# NOTE: This imports from the example answer key. To use YOUR OWN Part 3 code
# instead, change the two lines below:
#   sys.path.insert(0, str(Path(__file__).resolve().parent.parent / "docling-exercise"))
#   from docling_part3_exercise import (  â† rename your file to use underscores!)
# (Python can't import filenames with hyphens, so rename
#  docling-part3-exercise.py â†’ docling_part3_exercise.py first)
sys.path.insert(0, str(Path(__file__).resolve().parent.parent / "docling-exercise-example-answers"))

from docling_part3_answer import (  # â† change to docling_part3_exercise to use YOUR code (see NOTE above)
    _connect_db, _hybrid_search,
    _add_embeddings, _create_indexes,
)
from openai import OpenAI

# â”€â”€ Remote GPU Models (provided â€” no changes needed) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
chat_client = OpenAI(
    base_url="https://dev-8--vllm-qwen3-vl-8b-serve.modal.run/v1",
    api_key="not-needed",
)
CHAT_MODEL = "qwen3-vl-8b"

# â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_DB = Path(__file__).resolve().parent.parent / "docling-exercise-example-answers" / "output" / "rag_chunks.duckdb"
MAX_ROUNDS = 3  # Maximum reflection rounds


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TODO 1 (â˜…â˜…): Build generate_draft()  â€” ~15 lines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Generate an initial answer from retrieved context using the remote LLM.
# This is similar to _generate_answer() from Part 3, but returns the text
# instead of printing it, so we can feed it to the reflection step.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_draft(query: str, chunks: list[dict],
                   previous_answer: str = "") -> str:
    """Generate an answer draft from context. Optionally improve a previous answer."""

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  TODO 1: Generate a draft answer using the remote LLM.       â”‚
    # â”‚                                                              â”‚
    # â”‚  Step A: Build context from chunks                           â”‚
    # â”‚    - context = "\n\n---\n\n".join(c["text"] for c in chunks) â”‚
    # â”‚                                                              â”‚
    # â”‚  Step B: Build the prompt                                    â”‚
    # â”‚    - If previous_answer is empty (first draft):              â”‚
    # â”‚      "Based on the context, answer: {query}\n\n              â”‚
    # â”‚       Context:\n{context}"                                   â”‚
    # â”‚    - If previous_answer exists (improvement round):          â”‚
    # â”‚      "Improve this answer: {previous_answer}\n\n             â”‚
    # â”‚       Additional context:\n{context}\n\n                     â”‚
    # â”‚       Question: {query}"                                     â”‚
    # â”‚                                                              â”‚
    # â”‚  Step C: Call chat_client.chat.completions.create()          â”‚
    # â”‚    - Return resp.choices[0].message.content                  â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    raise NotImplementedError("TODO 1: implement generate_draft()")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TODO 2 (â˜…â˜…â˜…): Build reflect_on_answer()  â€” ~15 lines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# The LLM critiques its OWN answer. This is the key Self-RAG innovation.
# Returns a tuple: (is_sufficient: bool, critique: str)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def reflect_on_answer(query: str, answer: str) -> tuple[bool, str]:
    """LLM reflects on its own answer. Returns (is_sufficient, critique)."""

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  TODO 2: Implement self-reflection.                          â”‚
    # â”‚                                                              â”‚
    # â”‚  Step A: Build a reflection prompt                           â”‚
    # â”‚    - "Question: {query}\nAnswer: {answer}\n\n                â”‚
    # â”‚       Evaluate this answer:                                  â”‚
    # â”‚       1. Does it fully address the question?                 â”‚
    # â”‚       2. Are there any gaps or inaccuracies?                 â”‚
    # â”‚       3. What additional information would improve it?       â”‚
    # â”‚                                                              â”‚
    # â”‚       Start with SUFFICIENT or INSUFFICIENT,                 â”‚
    # â”‚       then explain why."                                     â”‚
    # â”‚                                                              â”‚
    # â”‚  Step B: Call chat_client.chat.completions.create()          â”‚
    # â”‚    - Extract reflection text                                 â”‚
    # â”‚                                                              â”‚
    # â”‚  Step C: Parse the response                                  â”‚
    # â”‚    - is_sufficient = reflection starts with "SUFFICIENT"     â”‚
    # â”‚    - Return (is_sufficient, reflection)                      â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    raise NotImplementedError("TODO 2: implement reflect_on_answer()")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TODO 3 (â˜…â˜…â˜…â˜…): Build self_rag()  â€” ~20 lines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# The Self-RAG loop: generate â†’ reflect â†’ if insufficient, retrieve more â†’ improve.
#
# Loop logic:
#   1. Retrieve initial context
#   2. Generate draft answer
#   3. Reflect on the answer
#   4. If SUFFICIENT â†’ done!
#   5. If INSUFFICIENT â†’ retrieve more context, generate improved answer
#   6. Repeat up to MAX_ROUNDS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def self_rag(conn, query: str, max_rounds: int = 3,
             top_k: int = 5) -> str:
    """Self-RAG loop: generate â†’ reflect â†’ improve until sufficient."""

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  TODO 3: Implement the Self-RAG loop.                        â”‚
    # â”‚                                                              â”‚
    # â”‚  Step A: Initial retrieval                                   â”‚
    # â”‚    - chunks = _hybrid_search(conn, query, top_k=top_k)       â”‚
    # â”‚    - all_chunks = list(chunks)  # accumulate across rounds   â”‚
    # â”‚                                                              â”‚
    # â”‚  Step B: Loop for max_rounds                                 â”‚
    # â”‚    for round_num in range(1, max_rounds + 1):                â”‚
    # â”‚                                                              â”‚
    # â”‚    - Generate: answer = generate_draft(query, all_chunks,    â”‚
    # â”‚                                        previous_answer)      â”‚
    # â”‚      Print: "ğŸ“ Round {round_num} draft: {answer[:100]}..."  â”‚
    # â”‚                                                              â”‚
    # â”‚    - Reflect: sufficient, critique = reflect_on_answer(      â”‚
    # â”‚                                        query, answer)        â”‚
    # â”‚      Print: "ğŸª Reflection: {critique[:100]}..."             â”‚
    # â”‚                                                              â”‚
    # â”‚    - If sufficient: print "âœ… Sufficient!" and return answer â”‚
    # â”‚                                                              â”‚
    # â”‚    - If not sufficient and not last round:                   â”‚
    # â”‚      Retrieve more: new = _hybrid_search(conn, query, top_k)â”‚
    # â”‚      all_chunks.extend(new)                                  â”‚
    # â”‚      previous_answer = answer                                â”‚
    # â”‚                                                              â”‚
    # â”‚  Step C: Return final answer after all rounds                â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    raise NotImplementedError("TODO 3: implement self_rag()")



# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TODO 4 (â˜…): Wire up CLI  â€” ~15 lines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Add --query and --max-rounds flags.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main() -> int:
    parser = argparse.ArgumentParser(
        description="ğŸ”´ Path C: Self-RAG â€” Generate â†’ Reflect â†’ Improve"
    )
    parser.add_argument("--db", type=Path, default=DEFAULT_DB)
    parser.add_argument("--query", type=str, required=True)

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  TODO 4: Add --max-rounds flag and wire up the pipeline.     â”‚
    # â”‚                                                              â”‚
    # â”‚  --max-rounds: type=int, default=3                           â”‚
    # â”‚    (maximum number of generateâ†’reflect cycles)               â”‚
    # â”‚                                                              â”‚
    # â”‚  Then:                                                       â”‚
    # â”‚  - answer = self_rag(conn, query, max_rounds)                â”‚
    # â”‚  - Print the final answer                                    â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    args = parser.parse_args()

    try:
        conn = _connect_db(args.db)
        _add_embeddings(conn)
        _create_indexes(conn)

        # TODO 4: Call self_rag() and print the answer here
        raise NotImplementedError("TODO 4: wire up CLI")

        conn.close()
        return 0
    except NotImplementedError as exc:
        print(f"\nâš ï¸  {exc}", file=sys.stderr)
        return 1
    except Exception as exc:
        print(f"\nâŒ Error: {exc}", file=sys.stderr)
        return 1


if __name__ == "__main__":
    raise SystemExit(main())