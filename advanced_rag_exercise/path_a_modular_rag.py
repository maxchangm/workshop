#!/usr/bin/env python3
"""ğŸŸ¡ Path A: Modular RAG â€” Medium Â· ~80 new lines

Build a smart router that classifies queries and picks the best search strategy.

You will:
  1. Build hyde_search()        â€” generate a fake answer, embed it, search (same as Path D)
  2. Build multi_query_search() â€” generate variants, search all, RRF merge (same as Path D)
  3. Build classify_query()     â€” LLM classifies query as factual/conceptual/ambiguous
  4. Build modular_rag()        â€” router that picks the best strategy per query type
  5. Wire up CLI                â€” --query flag, automatic routing

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
REUSING YOUR PART 1-3 CODE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
This file imports functions YOU built in Parts 1-3:
  â€¢ _search_bm25(conn, query, limit)           â€” keyword search (Part 3, TODO 13)
  â€¢ _search_vector(conn, query_vec, limit)      â€” cosine similarity (Part 3, TODO 14)
  â€¢ _hybrid_search(conn, query, top_k)          â€” RRF fusion (Part 3, TODO 16)
  â€¢ _generate_answer(query, chunks)             â€” LLM generation (Part 3, TODO 17)
  â€¢ ollama.embed(model, input)                  â€” local embeddings (Part 3, TODO 12)
  â€¢ RRF_K                                       â€” RRF constant k=60

NEW TODAY (Remote GPU):
  â€¢ chat_client.chat.completions.create()       â€” remote 8B LLM for classification
  â€¢ _rerank(query, documents, top_k)            â€” remote reranker for scoring

Run with (from the workshop/ folder):
    uv run --no-project --with duckdb --with ollama --with openai --with requests \
        advanced_rag_exercise/path_a_modular_rag.py \
        --query "How does attention work in transformers?"

Prerequisites:
  â€¢ workshop/docling_part3.py must exist (cp your Part 3 exercise)
  â€¢ workshop/output/rag_chunks.duckdb from Part 2
  â€¢ Ollama running with qwen3-embedding:0.6b
"""

from __future__ import annotations

import argparse
import sys
import time
from pathlib import Path

# â”€â”€ Make Part 3 importable â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sys.path.insert(0, str(Path(__file__).resolve().parent.parent / "workshop--example-answers"))

from docling_part3_answer import (
    _connect_db, _search_vector, _search_bm25,
    _hybrid_search, _generate_answer,
    _add_embeddings, _create_indexes,
    EMBED_MODEL, RRF_K,
)
import ollama
from openai import OpenAI
import requests

# â”€â”€ Remote GPU Models (provided â€” no changes needed) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
chat_client = OpenAI(
    base_url="https://dev-8--vllm-qwen3-vl-8b-serve.modal.run/v1",
    api_key="not-needed",
)
CHAT_MODEL = "qwen3-vl-8b"

RERANK_URL = "https://dev-8--qwen3-vl-reranker-2b-serve.modal.run/v1/rerank"
RERANK_MODEL = "qwen3-vl-reranker-2b"

def _rerank(query: str, documents: list[str], top_k: int = 3) -> list[tuple[str, float]]:
    """Rerank documents using remote GPU reranker. Returns [(text, score), ...]."""
    resp = requests.post(RERANK_URL, json={
        "model": RERANK_MODEL, "query": query,
        "documents": documents, "top_n": top_k,
    })
    results = resp.json()["results"]
    return [(r["document"], r["relevance_score"]) for r in results]

# â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_DB = Path(__file__).resolve().parent.parent / "output" / "rag_chunks.duckdb"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TODO 1 (â˜…â˜…): Build hyde_search()  â€” ~15 lines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HyDE = Hypothetical Document Embeddings.
# A short query produces a vague embedding. A fake 200-word answer produces
# a SPECIFIC embedding closer to real documents in vector space.
#
# You REUSE: ollama.embed(), _search_vector()
# You USE NEW: chat_client.chat.completions.create()
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def hyde_search(conn, query: str, top_k: int = 5) -> list[dict]:
    """Search using HyDE: generate fake answer â†’ embed it â†’ vector search."""

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  TODO 1: Implement HyDE search.                              â”‚
    # â”‚                                                              â”‚
    # â”‚  Step A: Generate a hypothetical answer using remote LLM     â”‚
    # â”‚    - chat_client.chat.completions.create(model=CHAT_MODEL,   â”‚
    # â”‚      messages=[{"role":"user","content":"Write a short       â”‚
    # â”‚      passage that answers: {query}"}])                       â”‚
    # â”‚    - Extract: resp.choices[0].message.content                â”‚
    # â”‚                                                              â”‚
    # â”‚  Step B: Embed the FAKE ANSWER (not the query!)              â”‚
    # â”‚    - ollama.embed(model=EMBED_MODEL, input=fake_answer)      â”‚
    # â”‚    - Get vector: resp["embeddings"][0]                       â”‚
    # â”‚                                                              â”‚
    # â”‚  Step C: Search with the fake answer's embedding             â”‚
    # â”‚    - _search_vector(conn, hyde_vec, limit=top_k)             â”‚
    # â”‚                                                              â”‚
    # â”‚  Step D: Fetch text for each result                          â”‚
    # â”‚    - conn.execute("SELECT chunk_id, text FROM rag_chunks     â”‚
    # â”‚      WHERE chunk_id = ?", [cid]).fetchone()                  â”‚
    # â”‚    - Return [{"chunk_id": ..., "text": ..., "score": ...}]   â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    raise NotImplementedError("TODO 1: implement hyde_search()")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TODO 2 (â˜…â˜…â˜…): Build multi_query_search()  â€” ~25 lines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Generate 3 query variants, search with ALL of them, fuse with RRF.
# Same RRF algorithm you built in Part 3!
#
# You REUSE: _hybrid_search(), RRF_K
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def multi_query_search(conn, query: str, top_k: int = 5) -> list[dict]:
    """Search using Multi-Query: generate variants â†’ search all â†’ RRF merge."""

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  TODO 2: Implement Multi-Query search.                       â”‚
    # â”‚                                                              â”‚
    # â”‚  Step A: Generate 3 query variants using remote LLM          â”‚
    # â”‚    - Prompt: "Generate 3 different search queries for:       â”‚
    # â”‚      {query}\nReturn one query per line. No numbering."      â”‚
    # â”‚    - Split by newlines, take first 3 non-empty lines         â”‚
    # â”‚    - all_queries = [query] + variants                        â”‚
    # â”‚                                                              â”‚
    # â”‚  Step B: Search with each variant using _hybrid_search()     â”‚
    # â”‚    - For each q: results = _hybrid_search(conn, q, top_k)   â”‚
    # â”‚    - Accumulate RRF: fused[cid] += 1/(RRF_K + rank + 1)    â”‚
    # â”‚                                                              â”‚
    # â”‚  Step C: Sort by fused score, fetch text, return top_k       â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    raise NotImplementedError("TODO 2: implement multi_query_search()")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TODO 3 (â˜…â˜…â˜…): Build classify_query()  â€” ~15 lines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# The BRAIN of Modular RAG. Classify the query so we can route it.
#
# Query types:
#   "factual"    â†’ specific fact lookup  â†’ BM25 is best (exact keyword match)
#   "conceptual" â†’ explanation needed    â†’ HyDE is best (semantic understanding)
#   "ambiguous"  â†’ vague/short query     â†’ Multi-Query (cast a wider net)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def classify_query(query: str) -> str:
    """Classify query as 'factual', 'conceptual', or 'ambiguous'."""

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  TODO 3: Classify the query type using the remote LLM.       â”‚
    # â”‚                                                              â”‚
    # â”‚  Step A: Build a classification prompt                       â”‚
    # â”‚    - System: "Classify the query as one of: factual,         â”‚
    # â”‚      conceptual, ambiguous. Reply with ONLY the word."       â”‚
    # â”‚    - User: the query                                         â”‚
    # â”‚                                                              â”‚
    # â”‚  Step B: Call chat_client.chat.completions.create()          â”‚
    # â”‚    - model=CHAT_MODEL, messages=[system_msg, user_msg]       â”‚
    # â”‚    - Extract: resp.choices[0].message.content.strip().lower()â”‚
    # â”‚                                                              â”‚
    # â”‚  Step C: Validate â€” if result not in expected set,           â”‚
    # â”‚    default to "ambiguous"                                    â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    raise NotImplementedError("TODO 3: implement classify_query()")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TODO 4 (â˜…â˜…): Build modular_rag()  â€” ~20 lines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# The ROUTER: classify â†’ pick strategy â†’ search â†’ rerank â†’ generate.
#
# Routing table:
#   "factual"    â†’ _search_bm25() + rerank
#   "conceptual" â†’ hyde_search()  + rerank
#   "ambiguous"  â†’ multi_query_search() + rerank
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def modular_rag(conn, query: str, top_k: int = 5) -> list[dict]:
    """Smart router: classify query â†’ pick best strategy â†’ search â†’ rerank."""

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  TODO 4: Implement the modular RAG router.                   â”‚
    # â”‚                                                              â”‚
    # â”‚  Step A: Classify the query                                  â”‚
    # â”‚    - qtype = classify_query(query)                           â”‚
    # â”‚    - Print: "ğŸ“‹ Query type: {qtype}"                         â”‚
    # â”‚                                                              â”‚
    # â”‚  Step B: Route to the best strategy                          â”‚
    # â”‚    - "factual"    â†’ BM25 search (exact keywords)             â”‚
    # â”‚      rows = _search_bm25(conn, query, limit=top_k*2)        â”‚
    # â”‚      candidates = fetch text for each chunk_id               â”‚
    # â”‚    - "conceptual" â†’ HyDE search (semantic)                   â”‚
    # â”‚      candidates = hyde_search(conn, query, top_k=top_k*2)   â”‚
    # â”‚    - "ambiguous"  â†’ Multi-Query (wide net)                   â”‚
    # â”‚      candidates = multi_query_search(conn, query, top_k*2)  â”‚
    # â”‚                                                              â”‚
    # â”‚  Step C: Rerank all candidates                               â”‚
    # â”‚    - texts = [c["text"] for c in candidates]                 â”‚
    # â”‚    - reranked = _rerank(query, texts, top_k=top_k)           â”‚
    # â”‚    - Return [{"text": t, "score": s} for t, s in reranked]  â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    raise NotImplementedError("TODO 4: implement modular_rag()")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TODO 5 (â˜…): Wire up CLI  â€” ~15 lines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Simple CLI: just --query. The router decides the strategy automatically!
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main() -> int:
    parser = argparse.ArgumentParser(
        description="ğŸŸ¡ Path A: Modular RAG â€” Smart Query Router"
    )
    parser.add_argument("--db", type=Path, default=DEFAULT_DB)
    parser.add_argument("--query", type=str, required=True)

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  TODO 5: Wire up the CLI.                                    â”‚
    # â”‚                                                              â”‚
    # â”‚  - Parse args                                                â”‚
    # â”‚  - Connect DB, add embeddings, create indexes                â”‚
    # â”‚  - Call modular_rag(conn, args.query)                        â”‚
    # â”‚  - Pass results to _generate_answer(args.query, results)     â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    args = parser.parse_args()

    try:
        conn = _connect_db(args.db)
        _add_embeddings(conn)
        _create_indexes(conn)

        # TODO 5: Call modular_rag() and _generate_answer() here
        raise NotImplementedError("TODO 5: wire up CLI")

        conn.close()
        return 0
    except NotImplementedError as exc:
        print(f"\nâš ï¸  {exc}", file=sys.stderr)
        return 1
    except Exception as exc:
        print(f"\nâŒ Error: {exc}", file=sys.stderr)
        return 1


if __name__ == "__main__":
    raise SystemExit(main())

