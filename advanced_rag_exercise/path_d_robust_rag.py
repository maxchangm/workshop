#!/usr/bin/env python3
"""ğŸŸ¢ Path D: Robust RAG â€” Easiest Â· ~60 new lines

Build HyDE + Multi-Query + Reranking into a polished, production-quality pipeline.

You will:
  1. Build hyde_search()       â€” generate a fake answer, embed it, search with its vector
  2. Build multi_query_search() â€” generate 3 query variants, search all, fuse with RRF
  3. Build robust_search()     â€” strategy wrapper with optional reranking + timing
  4. Wire up CLI               â€” --strategy and --rerank flags

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
REUSING YOUR PART 1-3 CODE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
This file imports functions YOU built in Parts 1-3:
  â€¢ _search_vector(conn, query_vec, limit)  â€” cosine similarity search (Part 3, TODO 14)
  â€¢ _hybrid_search(conn, query, top_k)      â€” RRF fusion of BM25+Vector (Part 3, TODO 16)
  â€¢ _generate_answer(query, chunks)         â€” LLM generation (Part 3, TODO 17)
  â€¢ ollama.embed(model, input)              â€” local embeddings (Part 3, TODO 12)
  â€¢ RRF_K                                   â€” RRF constant k=60 (Part 3)

NEW TODAY (Remote GPU):
  â€¢ chat_client.chat.completions.create()   â€” remote 8B LLM for generation/rewriting
  â€¢ _rerank(query, documents, top_k)        â€” remote reranker for scoring/filtering

Run with (from the workshop/ folder):
    uv run --no-project --with duckdb --with ollama --with openai --with requests \
        advanced_rag_exercise/path_d_robust_rag.py \
        --query "How does attention work in transformers?" \
        --strategy hyde --rerank

Prerequisites:
  â€¢ workshop/docling_part3.py must exist (cp your Part 3 exercise)
  â€¢ workshop/output/rag_chunks.duckdb from Part 2
  â€¢ Ollama running with qwen3-embedding:0.6b
"""

from __future__ import annotations

import argparse
import sys
import time
from pathlib import Path

# â”€â”€ Make Part 3 importable â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# The Part 3 answer key lives in workshop/workshop--example-answers/.
# We add that directory to Python's path so the import works.
sys.path.insert(0, str(Path(__file__).resolve().parent.parent / "workshop--example-answers"))

from docling_part3_answer import (   # Part 3 functions (local search)
    _connect_db, _search_vector, _search_bm25,
    _hybrid_search, _generate_answer,
    _add_embeddings, _create_indexes,
    EMBED_MODEL, RRF_K,
)
import ollama                         # Local embeddings
from openai import OpenAI             # Remote GPU chat
import requests                       # Remote GPU reranker

# â”€â”€ Remote GPU Models (provided â€” no changes needed) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# These point to Modal GPU endpoints. No API key required.
chat_client = OpenAI(
    base_url="https://dev-8--vllm-qwen3-vl-8b-serve.modal.run/v1",
    api_key="not-needed",
)
CHAT_MODEL = "qwen3-vl-8b"

RERANK_URL = "https://dev-8--qwen3-vl-reranker-2b-serve.modal.run/v1/rerank"
RERANK_MODEL = "qwen3-vl-reranker-2b"

def _rerank(query: str, documents: list[str], top_k: int = 3) -> list[tuple[str, float]]:
    """Rerank documents using remote GPU reranker. Returns [(text, score), ...]."""
    resp = requests.post(RERANK_URL, json={
        "model": RERANK_MODEL, "query": query,
        "documents": documents, "top_n": top_k,
    })
    results = resp.json()["results"]
    return [(r["document"], r["relevance_score"]) for r in results]

# â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_DB = Path(__file__).resolve().parent.parent / "output" / "rag_chunks.duckdb"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TODO 1 (â˜…â˜…): Build hyde_search()  â€” ~15 lines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HyDE = Hypothetical Document Embeddings.
# Insight: A short query ("attention mechanism?") produces a vague embedding.
#          A fake 200-word answer produces a SPECIFIC embedding that's closer
#          to real documents in vector space.
#
# You REUSE from Part 3:
#   â€¢ ollama.embed(model=EMBED_MODEL, input=text) â†’ {"embeddings": [[...]]}
#   â€¢ _search_vector(conn, query_vec, limit)       â†’ [(chunk_id, score), ...]
#
# You USE NEW today:
#   â€¢ chat_client.chat.completions.create(model=CHAT_MODEL, messages=[...])
#     â†’ resp.choices[0].message.content
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def hyde_search(conn, query: str, top_k: int = 5) -> list[dict]:
    """Search using HyDE: generate fake answer â†’ embed it â†’ vector search."""

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  TODO 1: Implement HyDE search.                              â”‚
    # â”‚                                                              â”‚
    # â”‚  Step A: Generate a hypothetical answer using the remote LLM â”‚
    # â”‚    - Use chat_client.chat.completions.create()               â”‚
    # â”‚    - Prompt: "Write a short passage that answers: {query}"   â”‚
    # â”‚    - Extract: resp.choices[0].message.content                â”‚
    # â”‚                                                              â”‚
    # â”‚  Step B: Embed the FAKE ANSWER (not the query!)              â”‚
    # â”‚    - ollama.embed(model=EMBED_MODEL, input=fake_answer)      â”‚
    # â”‚    - Get vector: resp["embeddings"][0]                       â”‚
    # â”‚                                                              â”‚
    # â”‚  Step C: Search with the fake answer's embedding             â”‚
    # â”‚    - _search_vector(conn, hyde_vec, limit=top_k)             â”‚
    # â”‚                                                              â”‚
    # â”‚  Step D: Fetch text for each result                          â”‚
    # â”‚    - conn.execute("SELECT chunk_id, text FROM rag_chunks     â”‚
    # â”‚      WHERE chunk_id = ?", [chunk_id]).fetchone()             â”‚
    # â”‚    - Return list of {"chunk_id": ..., "text": ..., "score":} â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    raise NotImplementedError("TODO 1: implement hyde_search()")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TODO 2 (â˜…â˜…â˜…): Build multi_query_search()  â€” ~25 lines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Multi-Query = cast a wider net. One query might miss documents due to
# vocabulary mismatch. Generate 3 variants, search with ALL of them,
# and fuse results using RRF â€” the SAME algorithm you built in Part 3!
#
# You REUSE from Part 3:
#   â€¢ _hybrid_search(conn, query, top_k)  â†’ [{"chunk_id", "text", "rrf_score"}]
#   â€¢ RRF_K constant (k=60)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def multi_query_search(conn, query: str, top_k: int = 5) -> list[dict]:
    """Search using Multi-Query: generate variants â†’ search all â†’ RRF merge."""

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  TODO 2: Implement Multi-Query search.                       â”‚
    # â”‚                                                              â”‚
    # â”‚  Step A: Generate 3 query variants using the remote LLM      â”‚
    # â”‚    - Prompt: "Generate 3 different search queries for:       â”‚
    # â”‚      {query}\nReturn one query per line. No numbering."      â”‚
    # â”‚    - Split response by newlines, take first 3                â”‚
    # â”‚    - Combine: all_queries = [query] + variants               â”‚
    # â”‚                                                              â”‚
    # â”‚  Step B: Search with each variant                            â”‚
    # â”‚    - For each q in all_queries:                               â”‚
    # â”‚      results = _hybrid_search(conn, q, top_k=top_k)         â”‚
    # â”‚    - Accumulate RRF scores across all searches:              â”‚
    # â”‚      fused_scores[cid] += 1.0 / (RRF_K + rank + 1)         â”‚
    # â”‚      (same RRF formula from Part 3!)                         â”‚
    # â”‚                                                              â”‚
    # â”‚  Step C: Sort by fused score, return top_k                   â”‚
    # â”‚    - Return [{"chunk_id": ..., "text": ..., "rrf_score": ...}]â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    raise NotImplementedError("TODO 2: implement multi_query_search()")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TODO 3 (â˜…â˜…): Build robust_search()  â€” ~20 lines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# The production wrapper: pick a strategy, optionally rerank, log timing.
#
# You REUSE from Part 3:
#   â€¢ _hybrid_search() as the default/fallback strategy
# You USE NEW today:
#   â€¢ _rerank(query, documents, top_k) â†’ [(text, score), ...]
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def robust_search(conn, query: str, strategy: str = "hyde",
                  use_rerank: bool = True, top_k: int = 5) -> list[dict]:
    """Production-quality search: strategy selection + optional reranking."""

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  TODO 3: Implement the strategy wrapper.                     â”‚
    # â”‚                                                              â”‚
    # â”‚  Step A: Record start time with time.time()                  â”‚
    # â”‚                                                              â”‚
    # â”‚  Step B: Strategy selection (if/elif/else)                   â”‚
    # â”‚    - "hyde"   â†’ hyde_search(conn, query, top_k=top_k*2)     â”‚
    # â”‚    - "multi"  â†’ multi_query_search(conn, query, top_k*2)    â”‚
    # â”‚    - else     â†’ _hybrid_search(conn, query, top_k=top_k*2)  â”‚
    # â”‚    (Fetch top_k*2 so reranker has more to work with)         â”‚
    # â”‚                                                              â”‚
    # â”‚  Step C: Optional reranking                                  â”‚
    # â”‚    - If use_rerank: extract texts, call _rerank()            â”‚
    # â”‚    - Build results: [{"text": t, "score": s} for t, s in ..]â”‚
    # â”‚    - If not reranking: just take first top_k candidates      â”‚
    # â”‚                                                              â”‚
    # â”‚  Step D: Log timing                                          â”‚
    # â”‚    - Print strategy name, rerank status, elapsed time        â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    raise NotImplementedError("TODO 3: implement robust_search()")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TODO 4 (â˜…): Wire up CLI  â€” ~15 lines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Add --strategy and --rerank flags. Same argparse pattern as Part 3.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main() -> int:
    parser = argparse.ArgumentParser(
        description="ğŸŸ¢ Path D: Robust RAG â€” HyDE + Multi-Query + Reranking"
    )
    parser.add_argument("--db", type=Path, default=DEFAULT_DB)
    parser.add_argument("--query", type=str, required=True)

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  TODO 4: Add --strategy and --rerank flags.                  â”‚
    # â”‚                                                              â”‚
    # â”‚  --strategy: choices=["hyde", "multi", "hybrid"]             â”‚
    # â”‚              default="hyde"                                   â”‚
    # â”‚  --rerank:   action="store_true" (flag, no value needed)     â”‚
    # â”‚                                                              â”‚
    # â”‚  Then call robust_search() with the parsed args and          â”‚
    # â”‚  pass results to _generate_answer().                         â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    args = parser.parse_args()

    try:
        conn = _connect_db(args.db)
        _add_embeddings(conn)
        _create_indexes(conn)

        # TODO 4: Call robust_search() and _generate_answer() here
        raise NotImplementedError("TODO 4: wire up CLI")

        conn.close()
        return 0
    except NotImplementedError as exc:
        print(f"\nâš ï¸  {exc}", file=sys.stderr)
        return 1
    except Exception as exc:
        print(f"\nâŒ Error: {exc}", file=sys.stderr)
        return 1


if __name__ == "__main__":
    raise SystemExit(main())
